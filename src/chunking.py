import os
import nltk
from nltk.tokenize import sent_tokenize
import pandas as pd
import logging
from typing import List, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def chunk_text_with_overlap(text: str, 
                            max_length: int = 768,
                            overlap: int = 192,
                            min_chunk_length: int = 50) -> List[str]:
    """
    –ß–∞–Ω–∫–∏–Ω–≥ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        min_chunk_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
        
    Returns:
        –°–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤
    """
    if not isinstance(text, str):
        if pd.isna(text):
            return []
        text = str(text)
    
    text = text.strip()
    if not text or len(text) < min_chunk_length:
        return []
    
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
    if len(text) <= max_length:
        return [text]
    
    try:
        sentences = sent_tokenize(text, language='russian')
    except:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –≤—Ä—É—á–Ω—É—é
        sentences = [s.strip() for s in text.split('.') if s.strip()]
    
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sent in sentences:
        sent_len = len(sent)
        
        # –ï—Å–ª–∏ –æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –±–æ–ª—å—à–µ max_length, —Ä–∞–∑–±–∏–≤–∞–µ–º –µ–≥–æ
        if sent_len > max_length:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
            if current_chunk:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ —á–∞—Å—Ç–∏
            words = sent.split()
            temp_chunk = []
            temp_length = 0
            
            for word in words:
                word_len = len(word) + 1  # +1 –¥–ª—è –ø—Ä–æ–±–µ–ª–∞
                if temp_length + word_len > max_length and temp_chunk:
                    chunks.append(' '.join(temp_chunk))
                    # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ: –±–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞
                    overlap_words = []
                    overlap_len = 0
                    for w in reversed(temp_chunk):
                        if overlap_len + len(w) + 1 <= overlap:
                            overlap_words.insert(0, w)
                            overlap_len += len(w) + 1
                        else:
                            break
                    temp_chunk = overlap_words
                    temp_length = overlap_len
                
                temp_chunk.append(word)
                temp_length += word_len
            
            if temp_chunk:
                chunks.append(' '.join(temp_chunk))
            continue
        
        # –û–±—ã—á–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        if current_length + sent_len + 1 <= max_length:
            current_chunk.append(sent)
            current_length += sent_len + 1
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # –°–æ–∑–¥–∞—ë–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ
            overlap_chunk = []
            overlap_length = 0
            
            for s in reversed(current_chunk):
                s_len = len(s) + 1
                if overlap_length + s_len <= overlap:
                    overlap_chunk.insert(0, s)
                    overlap_length += s_len
                else:
                    break
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π —á–∞–Ω–∫ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º
            current_chunk = overlap_chunk + [sent]
            current_length = overlap_length + sent_len + 1
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
    if current_chunk:
        chunk_text = ' '.join(current_chunk)
        if len(chunk_text) >= min_chunk_length:
            chunks.append(chunk_text)
    
    return chunks


def create_chunks_improved(input_path: str, 
                          output_path: str,
                          max_length: int = 512,
                          overlap: int = 128,
                          min_chunk_length: int = 50):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π
    
    Args:
        input_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞–Ω–∫–æ–≤
        max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
        overlap: –†–∞–∑–º–µ—Ä –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏—è
        min_chunk_length: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞
    """
    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {input_path}")
    df = pd.read_csv(input_path)
    
    logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
    logger.info(f"  max_length: {max_length}")
    logger.info(f"  overlap: {overlap}")
    logger.info(f"  min_chunk_length: {min_chunk_length}")
    
    data = []
    total_chunks = 0
    docs_with_chunks = 0
    
    for idx, row in df.iterrows():
        chunks = chunk_text_with_overlap(
            row['text'],
            max_length=max_length,
            overlap=overlap,
            min_chunk_length=min_chunk_length
        )
        
        if chunks:
            docs_with_chunks += 1
            for chunk in chunks:
                data.append({
                    'web_id': row['web_id'],
                    'text': chunk,
                    'chunk_length': len(chunk)
                })
                total_chunks += 1
        
        if (idx + 1) % 100 == 0:
            logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {idx + 1}/{len(df)}")
    
    df_chunks = pd.DataFrame(data)
    df_chunks.to_csv(output_path, index=False)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("\n" + "="*60)
    logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ß–ê–ù–ö–ò–ù–ì–ê:")
    logger.info(f"  –ò—Å—Ö–æ–¥–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(df)}")
    logger.info(f"  –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —á–∞–Ω–∫–∞–º–∏: {docs_with_chunks}")
    logger.info(f"  –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {total_chunks}")
    logger.info(f"  –°—Ä–µ–¥–Ω–µ–µ —á–∞–Ω–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {total_chunks/docs_with_chunks:.2f}")
    logger.info(f"  –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞: {df_chunks['chunk_length'].mean():.0f}")
    logger.info(f"  –ú–µ–¥–∏–∞–Ω–∞ –¥–ª–∏–Ω—ã —á–∞–Ω–∫–∞: {df_chunks['chunk_length'].median():.0f}")
    logger.info(f"  Min/Max –¥–ª–∏–Ω–∞: [{df_chunks['chunk_length'].min()}, {df_chunks['chunk_length'].max()}]")
    logger.info("="*60)
    
    return df_chunks


def analyze_chunking_strategy(input_path: str, 
                              max_lengths: List[int] = [256, 384, 512, 768],
                              overlaps: List[int] = [64, 128, 192]):
    """
    –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–∏–Ω–≥–∞
    """
    logger.info("\n" + "="*60)
    logger.info("–ê–ù–ê–õ–ò–ó –°–¢–†–ê–¢–ï–ì–ò–ô –ß–ê–ù–ö–ò–ù–ì–ê")
    logger.info("="*60)
    
    df = pd.read_csv(input_path)
    sample_text = df.iloc[0]['text']
    
    logger.info(f"\n–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ (–ø–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤):")
    logger.info(f"{sample_text[:500]}...\n")
    logger.info(f"–ü–æ–ª–Ω–∞—è –¥–ª–∏–Ω–∞: {len(sample_text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
    
    results = []
    
    for max_len in max_lengths:
        for overlap in overlaps:
            if overlap >= max_len * 0.5:  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω—ã
                continue
            
            chunks = chunk_text_with_overlap(sample_text, max_len, overlap)
            
            results.append({
                'max_length': max_len,
                'overlap': overlap,
                'num_chunks': len(chunks),
                'avg_length': sum(len(c) for c in chunks) / len(chunks) if chunks else 0,
                'overlap_ratio': overlap / max_len
            })
    
    results_df = pd.DataFrame(results)
    logger.info("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞:")
    logger.info(results_df.to_string(index=False))
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    logger.info("\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
    logger.info("  –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (<2000 —Å–∏–º–≤–æ–ª–æ–≤): max_length=512, overlap=128")
    logger.info("  –î–ª—è —Å—Ä–µ–¥–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (2000-10000): max_length=768, overlap=192")
    logger.info("  –î–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (>10000): max_length=1024, overlap=256")


if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='–£–ª—É—á—à–µ–Ω–Ω—ã–π —á–∞–Ω–∫–∏–Ω–≥ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º')
    parser.add_argument('--input', default='data/clean_answer_data.csv')
    parser.add_argument('--output', default='data/chunks.csv')
    parser.add_argument('--max-length', type=int, default=512)
    parser.add_argument('--overlap', type=int, default=128)
    parser.add_argument('--min-chunk-length', type=int, default=50)
    parser.add_argument('--analyze', action='store_true',
                       help='–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —á–∞–Ω–∫–∏–Ω–≥–∞')
    
    args = parser.parse_args()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º punkt –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        logger.info("–°–∫–∞—á–∏–≤–∞–Ω–∏–µ punkt tokenizer...")
        nltk.download('punkt')
    
    if args.analyze:
        analyze_chunking_strategy(args.input)
    else:
        create_chunks_improved(
            args.input,
            args.output,
            max_length=args.max_length,
            overlap=args.overlap,
            min_chunk_length=args.min_chunk_length
        )
